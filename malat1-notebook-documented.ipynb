{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f862d-cf2c-408a-b98b-f28c8abfdc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences are from:\n",
    "# https://pubmed.ncbi.nlm.nih.gov/19858363/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c9adb4-c892-46da-a812-0679ff9dadb7",
   "metadata": {},
   "source": [
    "# Documentation progress\n",
    "- ~~data import~~\n",
    "- ~~belgian plots~~\n",
    "- ~~skyline plots~~\n",
    "- ~~violin plots~~\n",
    "- ~~mutation calculation~~\n",
    "- ~~mutation plot~~\n",
    "- ~~mutation t-tests~~\n",
    "\n",
    "I believe this is done now, but haven't tested some of it. This code is rough and not in a shape intended for re-use on other projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a29faed-6ab9-46e9-afa3-8d25dbba91c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "from numpy import isnan, nan, sqrt\n",
    "from numpy import nanpercentile as percentile\n",
    "\n",
    "from scipy import stats as stats\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import matplotlib as mp\n",
    "mp.use('Agg')\n",
    "mp.rcParams[\"font.sans-serif\"].insert(0,\"Arial\")\n",
    "mp.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "mp.rcParams[\"pdf.fonttype\"] = 42 # use TrueType fonts when exporting PDFs\n",
    "                                 # (embeds most fonts - this is especially\n",
    "                                 #  useful when opening in Adobe Illustrator)\n",
    "mp.rcParams['font.size'] = 11\n",
    "mp.rcParams['xtick.direction'] = 'out'\n",
    "mp.rcParams['ytick.direction'] = 'out'\n",
    "mp.rcParams['legend.fontsize'] = 14\n",
    "mp.rcParams['grid.color'] = \".8\"\n",
    "mp.rcParams['grid.linestyle'] = '-'\n",
    "mp.rcParams['grid.linewidth'] = 1\n",
    "# mp.rcParams['figure.constrained_layout.use'] = True\n",
    "mp.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "rx_color = \"red\"\n",
    "bg_color = \"blue\"\n",
    "dc_color = \"darkgoldenrod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c4e6a-2562-4a93-826f-e90f7b89e8e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Creates Class CellType which represents a real world cell type and encapsulates any datasets that are generated from those cells\n",
    "\n",
    "the input 'files' here is a tuple with the form (ex, in), replicates = True means files should have 4 items in it, but is not required.\n",
    "the 'pps' input is the pairing probabilities and functions exactly like tables does\n",
    "\"\"\"\n",
    "\n",
    "class CellType: \n",
    "    def __init__(self, name, files=None, crop=None):\n",
    "            \n",
    "        if files is not None:\n",
    "            if len(files) == 4:\n",
    "                self.replicates = True\n",
    "            else:\n",
    "                self.replicates = False\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "        self._ex = SimpleNamespace()\n",
    "        self._in = SimpleNamespace()\n",
    "        \n",
    "        if crop is not None:\n",
    "            self._ex.table = pd.read_csv(files[0], sep=\"\\t\", usecols=[\"Nucleotide\", \"Sequence\", \"Norm_profile\", \"Norm_stderr\", \"Modified_effective_depth\"])[crop[0]:crop[1]]\n",
    "            self._in.table = pd.read_csv(files[1], sep=\"\\t\", usecols=[\"Nucleotide\", \"Sequence\", \"Norm_profile\", \"Norm_stderr\", \"Modified_effective_depth\"])[crop[0]:crop[1]]\n",
    "        else:\n",
    "            self._ex.table = pd.read_csv(files[0], sep=\"\\t\", usecols=[\"Nucleotide\", \"Sequence\", \"Norm_profile\", \"Norm_stderr\", \"Modified_effective_depth\"])\n",
    "            self._in.table = pd.read_csv(files[1], sep=\"\\t\", usecols=[\"Nucleotide\", \"Sequence\", \"Norm_profile\", \"Norm_stderr\", \"Modified_effective_depth\"])\n",
    "        \n",
    "        self._ex.name = self.name + \"-ex\"\n",
    "        self._in.name = self.name + \"-in\"\n",
    "        \n",
    "        for x in (self._ex, self._in):\n",
    "            x.seq = x.table.Sequence\n",
    "            x.num = x.table.Nucleotide\n",
    "            x.profile = np.array(x.table.Norm_profile)\n",
    "            x.stderr = np.array(x.table.Norm_stderr)\n",
    "            x.depth = np.array(x.table.Modified_effective_depth)\n",
    "            \n",
    "        if self.replicates:\n",
    "            self._ex_rep = SimpleNamespace()\n",
    "            self._in_rep = SimpleNamespace()\n",
    "            \n",
    "            if crop is not None:\n",
    "                self._ex_rep.table = pd.read_csv(files[2], sep=\"\\t\", usecols=[\"Nucleotide\", \"Sequence\", \"Norm_profile\", \"Norm_stderr\", \"Modified_effective_depth\"])[crop[0]:crop[1]]\n",
    "                self._in_rep.table = pd.read_csv(files[3], sep=\"\\t\", usecols=[\"Nucleotide\", \"Sequence\", \"Norm_profile\", \"Norm_stderr\", \"Modified_effective_depth\"])[crop[0]:crop[1]]\n",
    "            else:\n",
    "                self._ex_rep.table = pd.read_csv(files[2], sep=\"\\t\", usecols=[\"Nucleotide\", \"Sequence\", \"Norm_profile\", \"Norm_stderr\", \"Modified_effective_depth\"])\n",
    "                self._in_rep.table = pd.read_csv(files[3], sep=\"\\t\", usecols=[\"Nucleotide\", \"Sequence\", \"Norm_profile\", \"Norm_stderr\", \"Modified_effective_depth\"])\n",
    "            \n",
    "            self._ex_rep.name = self.name + \"-ex-replicate\"\n",
    "            self._in_rep.name = self.name + \"-in-replicate\"\n",
    "            \n",
    "            for x in (self._ex_rep, self._in_rep):\n",
    "                x.seq = x.table.Sequence\n",
    "                x.num = x.table.Nucleotide\n",
    "                x.profile = np.array(x.table.Norm_profile)\n",
    "                x.stderr = np.array(x.table.Norm_stderr)\n",
    "                x.depth = np.array(x.table.Modified_effective_depth)\n",
    "                x.med_adjusted = np.nanmedian(x.profile) - x.profile \n",
    "            \n",
    "            self.match_nans(self._in.profile, self._in_rep.profile)\n",
    "            self.match_nans(self._ex.profile, self._ex_rep.profile)\n",
    "            \n",
    "            # copy the original namespaces for where we're going to store the merged equivalent\n",
    "            self._ex_merged = SimpleNamespace(**self._ex.__dict__)\n",
    "            self._in_merged = SimpleNamespace(**self._in.__dict__)\n",
    "            \n",
    "            # average the replicate and the original for each profile into the merged namespace\n",
    "            self._ex_merged.profile = (self._ex.profile + self._ex_rep.profile)/2\n",
    "            self._in_merged.profile = (self._in.profile + self._in_rep.profile)/2\n",
    "            \n",
    "            # calculate the uncertainties\n",
    "            self._ex_merged.stderr = ((self._ex.stderr**2 + self._ex_rep.stderr**2)**.5)/2\n",
    "            self._in_merged.stderr = ((self._in.stderr**2 + self._in_rep.stderr**2)**.5)/2\n",
    "            \n",
    "            self._ex_merged.name = self._ex.name + \"-combined\"\n",
    "            self._in_merged.name = self._in.name + \"-combined\"\n",
    "            \n",
    "            # TODO: check if this depth calculation is kosher\n",
    "            self._ex_merged.depth = self._ex.depth + self._ex_rep.depth\n",
    "            self._in_merged.depth = self._in.depth + self._in_rep.depth\n",
    "            \n",
    "            # TODO: remove this, I don't think it's used anywhere? Check that and if not, it can go\n",
    "            self._ex_merged.med_adjusted = np.nanmedian(self._ex_merged.profile) - self._ex_merged.profile\n",
    "            self._in_merged.med_adjusted = np.nanmedian(self._in_merged.profile) - self._in_merged.profile\n",
    "    \n",
    "    ''' normalize the data using medians to the median value of the specified set (hek._in_merged, probably) '''\n",
    "    def median_normalize(self, norm_to):\n",
    "        \n",
    "        if self.replicates:\n",
    "            datasets = (self._in,\n",
    "                        self._in_rep,\n",
    "                        self._in_merged,\n",
    "                        self._ex,\n",
    "                        self._ex_rep,\n",
    "                        self._ex_merged,)\n",
    "        else:\n",
    "            datasets = (self._in,\n",
    "                        self._ex)\n",
    "            \n",
    "        for x in datasets:\n",
    "            x.med_normed = x.profile*(np.nanmedian(norm_to.profile)/np.nanmedian(x.profile))\n",
    "            # NOTE: is this the right thing to be doing with the errors? We're just scaling by the ratio so it should be fine, right?\n",
    "            x.med_normed_stderr = x.stderr*(np.nanmedian(norm_to.profile)/np.nanmedian(x.profile))\n",
    "        \n",
    "    \n",
    "    ''' output a .map file for the specified dataset into the specified folder, if mask is provided, mask values into np.nans to match each other '''\n",
    "    def to_map_file(self, dataset_string, fileout, data='profile', mask=None):\n",
    "        \n",
    "        if self.replicates:\n",
    "            datasets = {f'{self.name}-in': self._in,\n",
    "                        f'{self.name}-in-replicate': self._in_rep,\n",
    "                        f'{self.name}-in-combined': self._in_merged,\n",
    "                        f'{self.name}-ex': self._ex,\n",
    "                        f'{self.name}-ex-replicate': self._ex_rep,\n",
    "                        f'{self.name}-ex-combined': self._ex_merged}\n",
    "        else:\n",
    "            datasets = {f'{self.name}-in': self._in,\n",
    "                        f'{self.name}-ex': self._ex}\n",
    "            \n",
    "        dataset = datasets[dataset_string]\n",
    "        \n",
    "        if data == 'profile':\n",
    "            data = dataset.profile\n",
    "            errs = dataset.stderrs\n",
    "        elif data == 'med_normed':\n",
    "            data = dataset.med_normed\n",
    "            errs = dataset.med_normed_stderr\n",
    "        else:\n",
    "            raise Exception('unknown data table for map file conversion')\n",
    "        \n",
    "        # file-ify it into a .map file\n",
    "        with open(fileout, 'w') as f:\n",
    "            # each line of the map file has the following structure:\n",
    "            # num-->shape-->stderr-->nuc\n",
    "            for i, shape in enumerate(data):\n",
    "                \n",
    "                err = errs[i]\n",
    "                \n",
    "                # adjust our nan values to be what .map files expect\n",
    "                if np.isnan(shape):\n",
    "                    shape = '-999'\n",
    "                    err = '0'\n",
    "                \n",
    "                f.write(f\"{dataset.num[i]}\\t{shape}\\t{str(err)}\\t{dataset.seq[i]}\\n\")\n",
    "                \n",
    "    ''' match the nans or -999 values across two datasets, input should be some shape data'''            \n",
    "    @staticmethod\n",
    "    def match_nans(data1, data2):            \n",
    "        nan_mask = np.isnan(data1) | np.isnan(data2)\n",
    "        # just in case there's -999s as well, for whatever reason (we don't want these because they mess with calculations)\n",
    "        nan_mask = (data1 == -999) | (data2 == -999) | nan_mask\n",
    "\n",
    "        data1[nan_mask] = np.nan\n",
    "        data2[nan_mask] = np.nan\n",
    "        \n",
    "    \n",
    "    ''' add extra nans around the nan regions if a certain percentage of that window is nan'''\n",
    "    ''' outputs an array of tuples in the form (shape_data, errs), in the order rep1, rep2, merged for both in and ex data'''\n",
    "    def threshold_nans(self, window_size=51, nan_threshold=.25, dataset=\"profile\", inplace=False):\n",
    "        if dataset == \"med_normed\":\n",
    "            datasets = (\n",
    "                self._in.med_normed,\n",
    "                self._in_rep.med_normed,\n",
    "                self._in_merged.med_normed,\n",
    "                self._ex.med_normed,\n",
    "                self._ex_rep.med_normed,\n",
    "                self._ex_merged.med_normed\n",
    "            )\n",
    "            errs = (\n",
    "                self._in.med_normed_stderr,\n",
    "                self._in_rep.med_normed_stderr,\n",
    "                self._in_merged.med_normed_stderr,\n",
    "                self._ex.med_normed_stderr,\n",
    "                self._ex_rep.med_normed_stderr,\n",
    "                self._ex_merged.med_normed_stderr\n",
    "            )   \n",
    "        elif dataset == \"profile\":\n",
    "            datasets = (\n",
    "                self._in.profile,\n",
    "                self._in_rep.profile,\n",
    "                self._in_merged.profile,\n",
    "                self._ex.profile,\n",
    "                self._ex_rep.profile,\n",
    "                self._ex_merged.profile)\n",
    "            errs = (\n",
    "                self._in.stderr,\n",
    "                self._in_rep.stderr,\n",
    "                self._in_merged.stderr,\n",
    "                self._ex.stderr,\n",
    "                self._ex_rep.stderr,\n",
    "                self._ex_merged.stderr)\n",
    "        else:\n",
    "            raise Exception(f\"invalid dataset string: {dataset}\")\n",
    "            \n",
    "        # initiate the output array\n",
    "        output = [None]*len(datasets)\n",
    "            \n",
    "        # for each dataset and errorset\n",
    "        for i, (data, err) in enumerate(zip(datasets, errs)):\n",
    "            # there is probably a more elegant way to do this, but this is fine\n",
    "            if not inplace:\n",
    "                data = data.copy()\n",
    "                err = err.copy()\n",
    "            \n",
    "            # create a list of the ratios of nans in that window\n",
    "            nan_ratios = np.array([np.count_nonzero(np.isnan(data[j-(window_size//2):j+(window_size)//2]))/window_size for j in range(len(data))])\n",
    "            nan_mask = [nan_ratios > .25]\n",
    "            \n",
    "            # every true position in nan_mask should be a nan in both datasets\n",
    "            data[nan_mask] = np.nan\n",
    "            err[nan_mask] = np.nan\n",
    "\n",
    "            output[i] = (data, err)\n",
    "        \n",
    "        return output\n",
    "                \n",
    "                \n",
    "            \n",
    "dir_profiles = \"~/compare_reactivities/profiles\"                    \n",
    "\n",
    "vero_files = (\n",
    "    os.path.join(dir_profiles, \"MALAT1-vero-amp-all-ex_CHLSAB2_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-vero-amp-all-in_CHLSAB2_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-vero-amp-all-replicate-ex_CHLSAB2_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-vero-amp-all-replicate-in_CHLSAB2_MALAT1_profile.txt\")\n",
    ")\n",
    "hek_files = (\n",
    "    os.path.join(dir_profiles, \"MALAT1-hek-amp-all-ex_HUMAN_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-hek-amp-all-in_HUMAN_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-hek-amp-all-replicate-ex_HUMAN_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-hek-amp-all-replicate-in_HUMAN_MALAT1_profile.txt\")\n",
    ")\n",
    "a549_files = (\n",
    "    os.path.join(dir_profiles, \"MALAT1-a549-amp-ex-all-1_HUMAN_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-a549-amp-in-all-1_HUMAN_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-a549-amp-all-replicate-ex_HUMAN_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-a549-amp-all-replicate-in_HUMAN_MALAT1_profile.txt\")\n",
    ")\n",
    "\n",
    "# altered shapemapper runs\n",
    "# NOTE: temporarily adding ex and ex_rep to these even though that doesn't exist, just filling it in with the original ex as a stand-in\n",
    "hek_a_files = (\n",
    "    os.path.join(dir_profiles, \"MALAT1-hek-amp-all-ex_HUMAN_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-hek-amp-all-in-altered_HUMAN_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-hek-amp-all-ex_HUMAN_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-hek-amp-all-replicate-in-altered_HUMAN_MALAT1_profile.txt\")\n",
    ")\n",
    "vero_a_files = (\n",
    "    os.path.join(dir_profiles, \"MALAT1-hek-amp-all-ex_HUMAN_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-vero-amp-all-in-altered_CHLSAB2_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-hek-amp-all-ex_HUMAN_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-vero-amp-all-replicate-in-altered_CHLSAB2_MALAT1_profile.txt\")\n",
    ")\n",
    "a549_a_files = (\n",
    "    os.path.join(dir_profiles, \"MALAT1-hek-amp-all-ex_HUMAN_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-a549-amp-all-in-altered_HUMAN_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-hek-amp-all-ex_HUMAN_MALAT1_profile.txt\"),\n",
    "    os.path.join(dir_profiles, \"MALAT1-a549-amp-all-replicate-in-altered_HUMAN_MALAT1_profile.txt\")\n",
    ")\n",
    "\n",
    "# create each dataset to be used\n",
    "hek = CellType(\"hek\", hek_files)\n",
    "vero = CellType(\"vero\", vero_files)\n",
    "a549 = CellType(\"a549\", a549_files)\n",
    "hek_a = CellType(\"hek-a\", hek_a_files)\n",
    "vero_a = CellType(\"vero-a\", vero_a_files)\n",
    "a549_a = CellType(\"a549-a\", a549_a_files)\n",
    "\n",
    "# threshold all nans\n",
    "hek.threshold_nans(inplace=True)\n",
    "vero.threshold_nans(inplace=True)\n",
    "a549.threshold_nans(inplace=True)\n",
    "hek_a.threshold_nans(inplace=True)\n",
    "vero_a.threshold_nans(inplace=True)\n",
    "a549_a.threshold_nans(inplace=True)\n",
    "\n",
    "# normalize datasets to reference set (a549._in)\n",
    "hek.median_normalize(a549._in)\n",
    "vero.median_normalize(a549._in)\n",
    "a549.median_normalize(a549._in)\n",
    "hek_a.median_normalize(a549._in)\n",
    "vero_a.median_normalize(a549._in)\n",
    "a549_a.median_normalize(a549._in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5f47b-2a16-4afe-aee7-f4f0520d80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate .map files for use in superfold\n",
    "for x in (vero, hek, a549):\n",
    "    datasets = [x._in,\n",
    "                 x._in_rep,\n",
    "                 x._in_merged,\n",
    "                 x._ex,\n",
    "                 x._ex_rep,\n",
    "                 x._ex_merged]\n",
    "    \n",
    "    for d in datasets:\n",
    "        x.to_map_file(d.name, f'mapfiles/{d.name}_median-normalized-to-a549-in.map', data='med_normed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aeb110-8efc-483f-9a53-e6b48da23877",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' I have to define a standalone version of this for the a549/hek combined dataset (human)'''\n",
    "def simple_to_map_file(data, fileout):\n",
    "\n",
    "    stderr = data.med_normed_stderr\n",
    "    profile = data.med_normed\n",
    "    \n",
    "    # file-ify into .map file\n",
    "    with open(fileout, 'w') as f:\n",
    "        # each line of the map file has the following structure:\n",
    "        # num-->shape-->stderr-->nuc\n",
    "        for i, shape in enumerate(profile):\n",
    "\n",
    "            err = stderr[i]\n",
    "\n",
    "            # adjust our nan values to be what .map files expect\n",
    "            if np.isnan(shape):\n",
    "                shape = '-999'\n",
    "                err = '0'\n",
    "\n",
    "            f.write(f\"{data.num[i]}\\t{shape}\\t{str(err)}\\t{data.seq[i]}\\n\")\n",
    "            \n",
    "human = SimpleNamespace()\n",
    "human._ex = SimpleNamespace()\n",
    "\n",
    "human._ex.profile = (a549._ex.profile + hek._ex_rep.profile)/2\n",
    "human._ex.stderr = ((a549._ex.stderr**2 + hek._ex_rep.stderr**2)**.5)/2\n",
    "\n",
    "human._ex.med_normed = human._ex.profile*(np.nanmedian(a549._in.profile)/np.nanmedian(human._ex.profile))\n",
    "human._ex.med_normed_stderr = human._ex.stderr*(np.nanmedian(a549._in.profile)/np.nanmedian(human._ex.profile))\n",
    "\n",
    "human._ex.num = hek._ex.num\n",
    "human._ex.seq = hek._ex.seq\n",
    "\n",
    "human._ex.name = \"a549-ex-hek-ex-rep-merged\"\n",
    "\n",
    "human._ex.depth = hek._ex_rep.depth + a549._ex.depth\n",
    "\n",
    "simple_to_map_file(human._ex, f'mapfiles/{human._ex.name}_median-normalized-to-a549-in.map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae36ec1-9d1e-423b-a41c-85cb9148cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pairing probabilities for each dataset\n",
    "# commented lines are for deciding which are needed for the current analysis\n",
    "from RNAtools import dotPlot\n",
    "\n",
    "##### a549 #####\n",
    "\n",
    "# TODO: add a549 replicate pairing probs to these\n",
    "# a549._in.pprob = dotPlot(\"../structure_stuff/Superfold/results_a549-in_median-normalized-to-hek-in.map_44cc/merged_a549-in_median-normalized-to-hek-in.map_44cc.dp\").pairingProb()\n",
    "# a549._in_rep.pprob = dotPlot(\"../structure_stuff/Superfold/results_a549-in-replicate_median-normalized-to-hek-in.map_e2fd/merged_a549-in-replicate_median-normalized-to-hek-in.map_e2fd.dp\").pairingProb()\n",
    "# a549._ex.pprob = dotPlot(\"../structure_stuff/Superfold/results_a549-ex_median-normalized-to-hek-in.map_2bd9/merged_a549-ex_median-normalized-to-hek-in.map_2bd9.dp\").pairingProb()\n",
    "a549._in_merged.pprob = dotPlot(\"../structure_stuff/Superfold/results_a549-in-combined_median-normalized-to-a549-in.map_9329/merged_a549-in-combined_median-normalized-to-a549-in.map_9329.dp\").pairingProb()\n",
    "\n",
    "##### hek #####\n",
    "\n",
    "# hek._in.pprob = dotPlot(\"../structure_stuff/Superfold/results_hek-in_median-normalized-to-hek-in.map_c7a5/merged_hek-in_median-normalized-to-hek-in.map_c7a5.dp\").pairingProb()\n",
    "# hek._in_rep.pprob = dotPlot(\"../structure_stuff/Superfold/results_hek-in-replicate_median-normalized-to-hek-in.map_6e57/merged_hek-in-replicate_median-normalized-to-hek-in.map_6e57.dp\").pairingProb()\n",
    "hek._in_merged.pprob = dotPlot(\"../structure_stuff/Superfold/results_hek-in-combined_median-normalized-to-a549-in.map_5d74/merged_hek-in-combined_median-normalized-to-a549-in.map_5d74.dp\").pairingProb()\n",
    "\n",
    "# hek._ex.pprob = dotPlot(\"../structure_stuff/Superfold/results_hek-ex_median-normalized-to-hek-in.map_ced9/merged_hek-ex_median-normalized-to-hek-in.map_ced9.dp\").pairingProb()\n",
    "# hek._ex_rep.pprob = dotPlot(\"../structure_stuff/Superfold/results_hek-ex-replicate_median-normalized-to-hek-in.map_2fd2/merged_hek-ex-replicate_median-normalized-to-hek-in.map_2fd2.dp\").pairingProb()\n",
    "\n",
    "# OOPS: There is no hek._ex_merged data right now lol\n",
    "hek._ex_merged.pprob = dotPlot(\"../structure_stuff/Superfold/results_hek-ex-combined_median-normalized-to-a549-in.map_43ac/merged_hek-ex-combined_median-normalized-to-a549-in.map_43ac.dp\").pairingProb()\n",
    "\n",
    "##### vero #####\n",
    "\n",
    "# vero._in.pprob = dotPlot(\"../structure_stuff/Superfold/results_vero-in_median-normalized-to-hek-in.map_b2f6/merged_vero-in_median-normalized-to-hek-in.map_b2f6.dp\").pairingProb()\n",
    "# vero._in_rep.pprob = dotPlot(\"../structure_stuff/Superfold/results_vero-in-replicate_median-normalized-to-hek-in.map_f219/merged_vero-in-replicate_median-normalized-to-hek-in.map_f219.dp\").pairingProb()\n",
    "vero._in_merged.pprob = dotPlot(\"../structure_stuff/Superfold/results_vero-in-combined_median-normalized-to-a549-in.map_cf4c/merged_vero-in-combined_median-normalized-to-a549-in.map_cf4c.dp\").pairingProb()\n",
    "\n",
    "# vero._ex.pprob = dotPlot(\"../structure_stuff/Superfold/results_vero-ex_median-normalized-to-hek-in.map_f205/merged_vero-ex_median-normalized-to-hek-in.map_f205.dp\").pairingProb()\n",
    "# vero._ex_rep.pprob = dotPlot(\"../structure_stuff/Superfold/results_vero-ex-replicate_median-normalized-to-hek-in.map_1bb2/merged_vero-ex-replicate_median-normalized-to-hek-in.map_1bb2.dp\").pairingProb()\n",
    "vero._ex_merged.pprob = dotPlot(\"../structure_stuff/Superfold/results_vero-ex-combined_median-normalized-to-a549-in.map_9f3c/merged_vero-ex-combined_median-normalized-to-a549-in.map_9f3c.dp\").pairingProb()\n",
    "\n",
    "##### human #####\n",
    "human._ex.pprob = dotPlot(\"../structure_stuff/Superfold/results_a549-ex-hek-ex-rep-merged_median-normalized-to-a549-in.map_793a/merged_a549-ex-hek-ex-rep-merged_median-normalized-to-a549-in.map_793a.dp\").pairingProb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d2acc-0aee-42a2-9bea-0d2b4ffb2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' creates a boolean list of paired vs unpaired for each pairing probability set, ambiguous positions are nan '''\n",
    "\n",
    "datasets = (\n",
    "    a549._in_merged,\n",
    "    hek._in_merged,\n",
    "#     hek._ex_merged.pprob,\n",
    "    human._ex,\n",
    "    vero._in_merged,\n",
    "    vero._ex_merged\n",
    ")\n",
    "\n",
    "upperthresh = 0.95\n",
    "lowerthresh = 0.05\n",
    "\n",
    "# should I make a version of this in which ambiguous is a 0 instead? or maybe unpaired = -1 and paired = 1,\n",
    "for data in datasets:\n",
    "    probs = data.pprob\n",
    "    data.paired = np.empty(len(probs))\n",
    "    for i, p in enumerate(probs):\n",
    "        if p >= upperthresh:\n",
    "            data.paired[i] = True\n",
    "        elif p <= lowerthresh:\n",
    "            data.paired[i] = False\n",
    "        else:\n",
    "            data.paired[i] = np.nan\n",
    "    \n",
    "paired_list = []\n",
    "unpaired_list = []\n",
    "\n",
    "for paired, x in zip(a549._in_merged.paired, a549._in_merged.med_normed):\n",
    "    if np.isnan(paired):\n",
    "        continue\n",
    "        \n",
    "    if not paired:\n",
    "        unpaired_list += [x]\n",
    "    else:\n",
    "        paired_list += [x]\n",
    "        \n",
    "print(\"unpaired shape avg:\", round(np.nanmean(unpaired_list), 3))\n",
    "print(\"paired shape avg:\", round(np.nanmean(paired_list), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864adce-3813-4b78-9334-f41de8192509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' for each row in each table, get the absolute value of the difference in the relative reactivities '''\n",
    "\n",
    "# p1 and p2 are profiles from shapemapper\n",
    "def generate_reactivity_comp(seqs, p1, p2, absolute=False, errors=None):\n",
    "    # Get whichever is the larger of the two sequences to base our comparison on\n",
    "    seq_len = min(len(seqs[0]), len(seqs[1]))\n",
    "    \n",
    "    output = np.zeros(seq_len)\n",
    "    \n",
    "    for i in range(seq_len):\n",
    "        # grab values\n",
    "        v1 = p1[i]\n",
    "        v2 = p2[i]\n",
    "\n",
    "        # check if either value is NaN, if they are then ignore this row\n",
    "        if v1 and v2:\n",
    "            if absolute:\n",
    "                output[i] = abs(v1 - v2)\n",
    "            else:\n",
    "                output[i] = v1 - v2\n",
    "            \n",
    "            \n",
    "        else: output[i] = None\n",
    "    \n",
    "    # if errors were provided (as a tuple), generate a new set of propagated errors to return\n",
    "    if errors:\n",
    "        stderr = np.zeros(len(output))\n",
    "        for i in range(len(stderr)):\n",
    "            stderr[i] = sqrt(errors[0][i]**2 + errors[1][i]**2)\n",
    "        return (output, stderr)\n",
    "    else:    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c126dbae-d0b6-4671-9759-a49dfb6c7035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Inputs: profile, window size, and a nan threshold limit (as either an integer 0-100 or as a decimal 0-1). \n",
    "    Outputs: Altered profile with medians of the given window size'''\n",
    "\n",
    "def generate_rolling_medians(data, window, nan_limit=.1):\n",
    "    \n",
    "    # failsafe for when I inevitably forget to make the percentage be a decimal and write it as an int\n",
    "    if nan_limit > 1:\n",
    "        nan_limit = nan_limit/100\n",
    "    \n",
    "    # nan_limit is the largest acceptable percentage of NaN values in the window, 25% by default\n",
    "    \n",
    "    output = np.full(len(data), np.nan)\n",
    "    for i in range(window//2,len(data)-window//2):\n",
    "        data_window = data[i-window//2:i+window//2]\n",
    "        \n",
    "        # if nan_limit is None, then just ignore NaNs and calculate medians based on all other values in the window\n",
    "        if nan_limit is not None:\n",
    "            output[i] = np.nanmedian(data_window)\n",
    "        else:\n",
    "            # count up the number of NaN values in this segment, divide by window to get a percentage, and compare to the limit\n",
    "            if np.count_nonzero(np.isnan(data_window))/window > nan_limit:\n",
    "                output[i] = np.nan\n",
    "            else:\n",
    "                output[i] = np.nanmedian(data_window)\n",
    "    return output\n",
    "\n",
    "# medians = generate_rolling_medians(reactivity_comp, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2314866a-2603-40d1-a815-bdd9cabb2812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Count mutations by type and evaluate the shape data at that location '''\n",
    "\n",
    "# Transversions: A<->C, A<->T, G<->C, G<->T\n",
    "# Transitions: A<->G, C<->T\n",
    "# Insertions (relative to human): Human == '-', Vero != '-'\n",
    "# Deletions (relative to human): Human != '-', Vero == '-'\n",
    "\n",
    "def mutation_shapes(x, v, z):    \n",
    "#     nums_human = list(x.num)\n",
    "#     nums_vero = list(y.num)\n",
    "    \n",
    "    if (\"vero\" in x.name and \"vero\" not in v.name) or (\"vero\" not in x.name and \"vero\" in v.name):\n",
    "#         print(\"aligning...\")\n",
    "        alignment = SeqIO.parse(\"clustalo-I20210622-162928-0115-39849737-p1m.clustal_num\", 'clustal')\n",
    "\n",
    "        aligned_seqs = []\n",
    "\n",
    "        for record in alignment:\n",
    "            aligned_seqs += [str(record.seq)]\n",
    "\n",
    "        seqs = [np.array(list(aligned_seqs[0])), np.array(list(aligned_seqs[1]))]\n",
    "                \n",
    "                \n",
    "        aligned_profiles = [list(x.med_normed.copy()), list(v.med_normed.copy()), list(z.med_normed.copy())]\n",
    "        aligned_pairingprobs = [list(x.pprob.copy()), list(v.pprob.copy()), list(z.pprob.copy())]\n",
    "        aligned_paired = [list(x.paired.copy()), list(v.paired.copy()), list(z.paired.copy())]\n",
    "#         print(\"constructing aligned profiles...\")\n",
    "        for si, seq in enumerate(aligned_seqs):\n",
    "            for i in range(len(seq)):\n",
    "                if seq[i] == '-':\n",
    "                    aligned_profiles[si].insert(i, np.nan)\n",
    "                    aligned_pairingprobs[si].insert(i, np.nan)\n",
    "                    aligned_paired[si].insert(i, np.nan)        \n",
    "        # also do hek\n",
    "        for i, base in enumerate(aligned_seqs[0]):\n",
    "            if base == '-':\n",
    "                aligned_profiles[2].insert(i, np.nan)\n",
    "                aligned_pairingprobs[2].insert(i, np.nan)\n",
    "                aligned_paired[2].insert(i, np.nan)\n",
    "        \n",
    "        # make sure that nums is the same length now as the aligned sequence\n",
    "#         if (len(nums_human) != len(aligned_seqs[0])) or (len(nums_human) != len(aligned_seqs[0])):\n",
    "#             raise Exception(\"length of nums does not match sequence length\")\n",
    "    else:\n",
    "        # we should always be dealing with aligned data here, so throw an exception\n",
    "        raise Exception(\"datasets are not human and vero\")\n",
    "    \n",
    "    # shapes are stored here \n",
    "    mutation_shapes = {\n",
    "        f\"{x.name}\": {\n",
    "            \"transitions\": [],\n",
    "            \"transversions\": [],\n",
    "#             \"insertions\": [],\n",
    "            \"deletions\": [],\n",
    "            \"non-mutated\": [],\n",
    "            \"pairingprobs\": aligned_pairingprobs[0],\n",
    "            \"paired\": aligned_paired[0]\n",
    "        },\n",
    "        f\"{z.name}\": {\n",
    "            \"transitions\": [],\n",
    "            \"transversions\": [],\n",
    "#             \"insertions\": [],\n",
    "            \"deletions\": [],\n",
    "            \"non-mutated\": [],\n",
    "            \"pairingprobs\": aligned_pairingprobs[2],\n",
    "            \"paired\": aligned_paired[0]\n",
    "        },\n",
    "        f\"{v.name}\": {\n",
    "            \"transitions\": [],\n",
    "            \"transversions\": [],\n",
    "            \"insertions\": [],\n",
    "            \"non-mutated\": [],\n",
    "#             \"deletions\": []\n",
    "            \"pairingprobs\": aligned_pairingprobs[1],\n",
    "            \"paired\": aligned_paired[0]\n",
    "        }\n",
    "    }\n",
    "    # NOTE: quick and dirty implementation of pairing probs instead of shape data \n",
    "    for (shape_a, shape_h, shape_v), (base_h, base_v) in zip((zip(aligned_profiles[0], aligned_profiles[2], aligned_profiles[1])),(zip(seqs[0], seqs[1]))):\n",
    "#     for (shape_a, shape_h, shape_v), (base_h, base_v) in zip((zip(aligned_pairingprobs[0], aligned_pairingprobs[2], aligned_pairingprobs[1])),(zip(seqs[0], seqs[1]))):\n",
    "        # if there is a mutation\n",
    "        if (base_h != base_v):\n",
    "            # Transversion\n",
    "            if (base_h + base_v) in ('AC', 'AT', 'GC', 'GT', 'CA', 'TA', 'CG', 'TG'):\n",
    "                mutation_shapes[z.name]['transversions'].append(shape_h)\n",
    "                mutation_shapes[x.name]['transversions'].append(shape_a)\n",
    "                mutation_shapes[v.name]['transversions'].append(shape_v)\n",
    "                                                    \n",
    "            # Transitions\n",
    "            elif (base_h + base_v) in ('CT', 'TC', 'AG', 'GA'):\n",
    "                mutation_shapes[z.name]['transitions'].append(shape_h)\n",
    "                mutation_shapes[x.name]['transitions'].append(shape_a)\n",
    "                mutation_shapes[v.name]['transitions'].append(shape_v)\n",
    "                \n",
    "            # Insertions\n",
    "            elif base_h == '-' and base_v != '-':\n",
    "#                 mutation_shapes['human']['insertions'].append(shape_h)\n",
    "                mutation_shapes[v.name]['insertions'].append(shape_v)\n",
    "                \n",
    "            # Deletions\n",
    "            elif base_h != '-' and base_v == '-':\n",
    "                mutation_shapes[z.name]['deletions'].append(shape_h)\n",
    "                mutation_shapes[x.name]['deletions'].append(shape_a)\n",
    "#                 mutation_shapes['vero']['deletions'].append(shape_v)\n",
    "\n",
    "        else:\n",
    "            mutation_shapes[z.name]['non-mutated'].append(shape_h)\n",
    "            mutation_shapes[x.name]['non-mutated'].append(shape_a)\n",
    "            mutation_shapes[v.name]['non-mutated'].append(shape_v)\n",
    "                \n",
    "    return mutation_shapes\n",
    "#         temp = [None]*2\n",
    "#         temp[0] = aligned_seqs[1]\n",
    "#         temp[1] = aligned_seqs[0]\n",
    "#         aligned_seqs = temp\n",
    "\n",
    "            # now our profiles should match with our sequences, num needs to be updated\n",
    "    #         aligned_nums = [list(range(len(aligned_seqs[0]))), list(range(len(aligned_seqs[1])))]\n",
    "\n",
    "    #         nums = aligned_nums\n",
    "\n",
    "    #         diffs = np.array(abs(data[0].med_normed - data[1].med_normed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a0e42-9ab6-496b-9f2a-08abc6b1ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = a549._in_merged\n",
    "v = vero._in_merged\n",
    "z = hek._in_merged\n",
    "\n",
    "# x = human._ex\n",
    "# v = vero._ex_merged\n",
    "# z = hek._ex_merged\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "shapes = mutation_shapes(x, v, z)\n",
    "\n",
    "print('Shape values by mutation type (relative to human)- Mean, Standard Deviation, Median')\n",
    "print(f'{x.name} \\tMean\\tStd\\tMedian')\n",
    "print('\\tTransitions:\\t' + str(round(np.nanmean(shapes[x.name]['transitions']), 3)) + '\\t' + str(round(np.nanstd(shapes[x.name]['transitions']), 3)) + '\\t' + str(round(np.nanmedian(shapes[x.name]['transitions']), 3)))\n",
    "print('\\tTransversions:\\t' + str(round(np.nanmean(shapes[x.name]['transversions']), 3)) + '\\t' + str(round(np.nanstd(shapes[x.name]['transversions']), 3)) + '\\t' + str(round(np.nanmedian(shapes[x.name]['transversions']), 3)))\n",
    "print('\\tDeletions:\\t' + str(round(np.nanmean(shapes[x.name]['deletions']), 3)) + '\\t' + str(round(np.nanstd(shapes[x.name]['deletions']), 3)) + '\\t' + str(round(np.nanmedian(shapes[x.name]['deletions']), 3)))\n",
    "\n",
    "print(f'\\n{v.name} \\tMean\\tStd\\tMedian')\n",
    "print('\\tTransitions:\\t' + str(round(np.nanmean(shapes[v.name]['transitions']), 3)) + '\\t' + str(round(np.nanstd(shapes[v.name]['transitions']), 3)) + '\\t' + str(round(np.nanmedian(shapes[v.name]['transitions']), 3)))\n",
    "print('\\tTransversions:\\t' + str(round(np.nanmean(shapes[v.name]['transversions']), 3)) + '\\t' + str(round(np.nanstd(shapes[v.name]['transversions']), 3)) + '\\t' + str(round(np.nanmedian(shapes[v.name]['transversions']), 3)))\n",
    "print('\\tInsertions:\\t' + str(round(np.nanmean(shapes[v.name]['insertions']), 3)) + '\\t' + str(round(np.nanstd(shapes[v.name]['insertions']), 3)) + '\\t' + str(round(np.nanmedian(shapes[v.name]['insertions']), 3)))\n",
    "\n",
    "t_transitions = stats.ttest_ind(shapes[x.name]['transitions'], shapes[v.name]['transitions'], nan_policy='omit')\n",
    "t_transversions = stats.ttest_ind(shapes[x.name]['transversions'], shapes[v.name]['transversions'], nan_policy='omit')\n",
    "t_human = stats.ttest_ind(shapes[x.name]['transversions'], shapes[x.name]['transitions'], nan_policy='omit')\n",
    "t_vero = stats.ttest_ind(shapes[v.name]['transversions'], shapes[v.name]['transitions'], nan_policy='omit')\n",
    "\n",
    "print(f'\\nt-tests ({x.name} vs {v.name})')\n",
    "print('\\tTransitions:\\t t = ' + str(round(t_transitions[0], 5)) + '\\t p = ' + str(round(t_transitions[1], 5)))\n",
    "print('\\tTransversions:\\t t = ' + str(round(t_transversions[0], 5)) + '\\t p = ' + str(round(t_transversions[1], 5)))\n",
    "print('\\tHuman:\\t t = ' + str(round(t_human[0], 5)) + '\\t p = ' + str(round(t_human[1], 5)))\n",
    "print('\\tVero:\\t t = ' + str(round(t_vero[0], 5)) + '\\t p = ' + str(round(t_vero[1], 5)))\n",
    "# print('\\tInsertions:\\t' + str(stats.ttest_ind(shapes['human']['insertions'], shapes['vero']['insertions'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7676d3e-12ce-4e5c-81a6-61f96a733ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' count mutation types '''\n",
    "    \n",
    "print(x.name)\n",
    "print(f'\\tTransitions\\t' + str(len(shapes[x.name]['transitions'])))\n",
    "print(f'\\tTransversions\\t' + str(len(shapes[x.name]['transversions'])))\n",
    "print(f'\\tDeletions\\t' + str(len(shapes[x.name]['deletions'])))\n",
    "print()\n",
    "\n",
    "print(z.name)\n",
    "print(f'\\tTransitions:\\t' + str(len(shapes[z.name]['transitions'])))\n",
    "print(f'\\tTransversions:\\t' + str(len(shapes[z.name]['transversions'])))\n",
    "print(f'\\tDeletions:\\t' + str(len(shapes[z.name]['deletions'])))\n",
    "print()\n",
    "\n",
    "print(v.name)\n",
    "print(f'\\tTransitions:\\t' + str(len(shapes[v.name]['transitions'])))\n",
    "print(f'\\tTransversions:\\t' + str(len(shapes[v.name]['transversions'])))\n",
    "print(f'\\tInsertions:\\t' + str(len(shapes[v.name]['insertions'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54952b-042f-4bed-91a9-87ed92d4c1f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' generate violin box strip plots for the mutation types '''\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "non_h = np.array(shapes[z.name]['non-mutated'])\n",
    "non_a = np.array(shapes[x.name]['non-mutated'])\n",
    "# non_a = np.array(shapes['a549']['non-mutated'])\n",
    "non_v = np.array(shapes[v.name]['non-mutated'])\n",
    "\n",
    "mut_h = np.array(shapes[z.name]['transitions'] + shapes[z.name]['transversions'])\n",
    "mut_a = np.array(shapes[x.name]['transitions'] + shapes[x.name]['transversions'])\n",
    "mut_v = np.array(shapes[v.name]['transitions'] + shapes[v.name]['transversions'])\n",
    "\n",
    "tsi_h = np.array(shapes[z.name]['transitions'])\n",
    "tvr_h = np.array(shapes[z.name]['transversions'])\n",
    "\n",
    "tsi_a = np.array(shapes[x.name]['transitions'])\n",
    "tvr_a = np.array(shapes[x.name]['transversions'])\n",
    "\n",
    "tsi_v = np.array(shapes[v.name]['transitions'])\n",
    "tvr_v = np.array(shapes[v.name]['transversions'])\n",
    "\n",
    "del_h = np.array(shapes[z.name]['deletions'])\n",
    "del_a = np.array(shapes[x.name]['deletions'])\n",
    "ins_v = np.array(shapes[v.name]['insertions'])\n",
    "\n",
    "allmut_h = np.array(shapes[z.name]['transitions'] + shapes[z.name]['transversions'] + shapes[z.name]['deletions'])\n",
    "allmut_a = np.array(shapes[x.name]['transitions'] + shapes[x.name]['transversions'] + shapes[x.name]['deletions'])\n",
    "allmut_v = np.array(shapes[v.name]['transitions'] + shapes[v.name]['transversions'] + shapes[v.name]['insertions'])\n",
    "\n",
    "\n",
    "# kill all the nan values\n",
    "non_h = non_h[~np.isnan(non_h)]\n",
    "mut_h = mut_h[~np.isnan(mut_h)]\n",
    "del_h = del_h[~np.isnan(del_h)]\n",
    "tsi_h = tsi_h[~np.isnan(tsi_h)]\n",
    "tvr_h = tvr_h[~np.isnan(tvr_h)]\n",
    "allmut_h = allmut_h[~np.isnan(allmut_h)]\n",
    "\n",
    "non_a = non_a[~np.isnan(non_a)]\n",
    "mut_a = mut_a[~np.isnan(mut_a)]\n",
    "del_a = del_a[~np.isnan(del_a)]\n",
    "tsi_a = tsi_a[~np.isnan(tsi_a)]\n",
    "tvr_a = tvr_a[~np.isnan(tvr_a)]\n",
    "allmut_a = allmut_a[~np.isnan(allmut_a)]\n",
    "\n",
    "non_v = non_v[~np.isnan(non_v)]\n",
    "mut_v = mut_v[~np.isnan(mut_v)]\n",
    "ins_v = ins_v[~np.isnan(ins_v)]\n",
    "tsi_v = tsi_v[~np.isnan(tsi_v)]\n",
    "tvr_v = tvr_v[~np.isnan(tvr_v)]\n",
    "allmut_v = allmut_v[~np.isnan(allmut_v)]\n",
    "    \n",
    "# mean_non_h = np.mean(non_h)\n",
    "# mean_non_v = np.mean(non_v)\n",
    "# mean_mut_h = np.mean(mut_h)\n",
    "# mean_mut_v = np.mean(mut_v)\n",
    "\n",
    "######## plotting stuff ########\n",
    "\n",
    "left_inches = 0.9\n",
    "right_inches = 0.4\n",
    "fig_width = 22.5\n",
    "fig_height = 10\n",
    "num_panels = 3\n",
    "\n",
    "left_percent = left_inches/fig_width\n",
    "right_percent = 1-right_inches/fig_width\n",
    "\n",
    "fig = plt.figure(figsize=(fig_width,fig_height))\n",
    "\n",
    "#     create axes for subplots\n",
    "axes = []    \n",
    "for i in range(num_panels):\n",
    "    axes += [plt.subplot(1, num_panels, i + 1)]\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_ylim(-.1, 1.2)\n",
    "    \n",
    "# plt.subplots_adjust(hspace=0.5, left=left_percent,right=right_percent, top=0.98, bottom=0.02)\n",
    "\n",
    "# axes[0].boxplot((ins_v, del_h, mut_v, mut_h, non_v, non_h), vert=False, labels=(\"vero-in inserted\", \"a549-in deleted\", \"vero-in mutated\", \"a549-in mutated\", \"vero-in non-mutated\", \"a549-in non-mutated\"))\n",
    "\n",
    "# t_non = stats.ttest_ind(non_h, non_v)\n",
    "# t_mut = stats.ttest_ind(mut_h, mut_v)\n",
    "# t_indel = stats.ttest_ind(del_h, ins_v)\n",
    "# t_a549_non_vs_mut = stats.ttest_ind(non_h, mut_h)\n",
    "# t_vero_non_vs_mut = stats.ttest_ind(non_v, mut_v)\n",
    "# t_a549_non_vs_del = stats.ttest_ind(non_h, del_h)\n",
    "# t_vero_non_vs_ins = stats.ttest_ind(non_v, ins_v)\n",
    "# t_a549_del_vs_mut = stats.ttest_ind(del_h, mut_h)\n",
    "# t_vero_ins_vs_mut = stats.ttest_ind(ins_v, mut_v)\n",
    "\n",
    "# axes[0].boxplot((non_h, mut_h, del_h, tsi_h, tvr_h), labels=(\"non-mutated\", \"mutated\", \"deleted\", \"transition\", \"transversion\"))\n",
    "# axes[0].boxplot((non_a, mut_a), labels=(\"non-mutated\", \"mutated\"))\n",
    "# plt.sca(axes[0])\n",
    "# sns.violinplot(x=[\"non-mutated\", \"mutated\"], y=(non_a, mut_a))\n",
    "\n",
    "# axes[0].violinplot((non_a, mut_a))\n",
    "\n",
    "point_size = 2\n",
    "point_alpha = .1\n",
    "jitter = .25\n",
    "box_width = .15\n",
    "cut = 0\n",
    "gridsize = 500\n",
    "\n",
    "sns.violinplot(ax=axes[0], data=[non_a, allmut_a], rasterized=True, inner=None, color='lightgray', zorder=0, cut=cut, gridsize=gridsize)\n",
    "sns.stripplot(ax=axes[0], data=[non_a, allmut_a], alpha=point_alpha, rasterized=True, size=point_size, color='black', jitter = jitter, zorder=1)\n",
    "sns.boxplot(ax=axes[0], data=[non_a, allmut_a], width=box_width, zorder=2, showfliers=False, boxprops={'zorder': 2}, color='.6')\n",
    "# axes[0].set_title(\"a549-in-combined\")\n",
    "axes[0].set_title(x.name)\n",
    "# axes[0].set(xticklabels=('non-mutated', 'transition/transversion'))\n",
    "# axes[0].set(xticklabels=('non-mutated', 'deletion'))\n",
    "# axes[0].set(xticklabels=('transition/transversion', 'deletion'))\n",
    "axes[0].set(xticklabels=('non-mutated', 'mutated (any kind)'))\n",
    "# statistical annotation\n",
    "y, h = 1.1, .02\n",
    "axes[0].plot([0, 0, 1, 1], [y, y+h, y+h, y], lw=1.5, c='black')\n",
    "axes[0].text(.5, y+h, f\"p = {round(t_a549_non_vs_allmut[1], 3)}\", ha='center', va='bottom', color='black')\n",
    "\n",
    "sns.violinplot(ax=axes[1], data=[non_h, allmut_h], rasterized=True, inner=None, color='lightgray', zorder=0, cut=cut, gridsize=gridsize)\n",
    "sns.stripplot(ax=axes[1], data=[non_h, allmut_h], alpha=point_alpha, rasterized=True, size=point_size, color='black', jitter = jitter, zorder=1)\n",
    "sns.boxplot(ax=axes[1], data=[non_h, allmut_h], width=box_width, zorder=2, showfliers=False, boxprops={'zorder': 2}, color='.6')\n",
    "axes[1].set_title(z.name)\n",
    "# axes[1].set(xticklabels=('non-mutated', 'transition/transversion'))\n",
    "# axes[1].set(xticklabels=('non-mutated', 'deletion'))\n",
    "# axes[1].set(xticklabels=('transition/transversion', 'deletion'))\n",
    "axes[1].set(xticklabels=('non-mutated', 'mutated (any kind)'))\n",
    "# statistical annotation\n",
    "y, h = 1.1, .02\n",
    "axes[1].plot([0, 0, 1, 1], [y, y+h, y+h, y], lw=1.5, c='black')\n",
    "axes[1].text(.5, y+h, f\"p = {round(t_hek_non_vs_allmut[1], 3)}\", ha='center', va='bottom', color='black')\n",
    "\n",
    "sns.violinplot(ax=axes[2], data=[non_v, allmut_v], rasterized=True, inner=None, color='lightgray', zorder=0, cut=cut, gridsize=gridsize)\n",
    "sns.stripplot(ax=axes[2], data=[non_v, allmut_v], alpha=point_alpha, rasterized=True, size=point_size, color='black', jitter = jitter, zorder=1)\n",
    "sns.boxplot(ax=axes[2], data=[non_v, allmut_v], width=box_width, zorder=2, showfliers=False, boxprops={'zorder': 2}, color='.6')\n",
    "axes[2].set_title(v.name)\n",
    "# axes[2].set(xticklabels=('non-mutated', 'transition/transversion'))\n",
    "# axes[2].set(xticklabels=('non-mutated', 'insertion'))\n",
    "# axes[2].set(xticklabels=('transition/transversion', 'insertion'))\n",
    "axes[2].set(xticklabels=('non-mutated', 'mutated (any kind)'))\n",
    "\n",
    "# statistical annotation\n",
    "y, h = 1.1, .02\n",
    "axes[2].plot([0, 0, 1, 1], [y, y+h, y+h, y], lw=1.5, c='black')\n",
    "axes[2].text(.5, y+h, f\"p = {round(t_vero_non_vs_allmut[1], 3)}\", ha='center', va='bottom', color='black')\n",
    "\n",
    "# axes[1].boxplot((non_v, mut_v, ins_v, tsi_v, tvr_v), labels=(\"non-mutated\", \"mutated\", \"inserted\", \"transition\", \"transversion\"))\n",
    "# axes[1].set_title(\"vero-in-combined\")\n",
    "\n",
    "plt.savefig(f\"plots_boxplots/mutation_analysis_ex-vivo_rasterized_alpha020_withviolin_nonvsallmuts.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac4cd43-842b-4f7e-8296-f8f3a3a6e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Generate all of the t tests for the above cells '''\n",
    "\n",
    "t_non = stats.ttest_ind(non_h, non_v)\n",
    "t_mut = stats.ttest_ind(mut_h, mut_v)\n",
    "t_indel = stats.ttest_ind(del_h, ins_v)\n",
    "t_tsi = stats.ttest_ind(tsi_h, tsi_v)\n",
    "t_tvr = stats.ttest_ind(tvr_h, tvr_v)\n",
    "\n",
    "t_a549_non_vs_mut = stats.ttest_ind(non_a, mut_a)\n",
    "t_a549_non_vs_del = stats.ttest_ind(non_a, del_a)\n",
    "t_a549_non_vs_tsi = stats.ttest_ind(non_a, tsi_a)\n",
    "t_a549_non_vs_tvr = stats.ttest_ind(non_a, tsi_a)\n",
    "t_a549_del_vs_mut = stats.ttest_ind(del_a, mut_a)\n",
    "t_a549_tsi_vs_tvr = stats.ttest_ind(tsi_a, tvr_a)\n",
    "t_a549_non_vs_allmut = stats.ttest_ind(non_a, allmut_a)\n",
    "\n",
    "t_vero_non_vs_mut = stats.ttest_ind(non_v, mut_v)\n",
    "t_vero_non_vs_ins = stats.ttest_ind(non_v, ins_v)\n",
    "t_vero_non_vs_tsi = stats.ttest_ind(non_v, tsi_v)\n",
    "t_vero_non_vs_tvr = stats.ttest_ind(non_v, tsi_v)\n",
    "t_vero_ins_vs_mut = stats.ttest_ind(ins_v, mut_v)\n",
    "t_vero_tsi_vs_tvr = stats.ttest_ind(tsi_v, tvr_v)\n",
    "t_vero_non_vs_allmut = stats.ttest_ind(non_v, allmut_v)\n",
    "\n",
    "t_hek_non_vs_mut = stats.ttest_ind(non_h, mut_h)\n",
    "t_hek_non_vs_del = stats.ttest_ind(non_h, del_h)\n",
    "t_hek_non_vs_tsi = stats.ttest_ind(non_h, tsi_h)\n",
    "t_hek_non_vs_tvr = stats.ttest_ind(non_h, tsi_h)\n",
    "t_hek_del_vs_mut = stats.ttest_ind(del_h, mut_h)\n",
    "t_hek_tsi_vs_tvr = stats.ttest_ind(tsi_h, tvr_h)\n",
    "t_hek_non_vs_allmut = stats.ttest_ind(non_h, allmut_h)\n",
    "\n",
    "\n",
    "kw_a549_non_vs_mut = stats.kruskal(non_a, mut_a, nan_policy=\"omit\")\n",
    "kw_hek_non_vs_mut = stats.kruskal(non_h, mut_h, nan_policy=\"omit\")\n",
    "kw_vero_non_vs_mut = stats.kruskal(non_v, mut_v, nan_policy=\"omit\")\n",
    "\n",
    "ks_a549_non_vs_mut = stats.kstest(non_a, mut_a)\n",
    "ks_hek_non_vs_mut = stats.kstest(non_h, mut_h)\n",
    "ks_vero_non_vs_mut = stats.kstest(non_v, mut_v)\n",
    "\n",
    "print(\"----- T-TESTS -----\")\n",
    "print(\"a549-in\")\n",
    "print(\"\\tNon vs Mut:\\t t = \" + str(round(t_a549_non_vs_mut[0], 5)) + \"\\t p = \" + str(round(t_a549_non_vs_mut[1], 5)))\n",
    "print(\"\\tNon vs Del:\\t t = \" + str(round(t_a549_non_vs_del[0], 5)) + \"\\t p = \" + str(round(t_a549_non_vs_del[1], 5)))\n",
    "print(\"\\tNon vs Tsi:\\t t = \" + str(round(t_a549_non_vs_tsi[0], 5)) + \"\\t p = \" + str(round(t_a549_non_vs_tsi[1], 5)))\n",
    "print(\"\\tNon vs Tvr:\\t t = \" + str(round(t_a549_non_vs_tvr[0], 5)) + \"\\t p = \" + str(round(t_a549_non_vs_tvr[1], 5)))\n",
    "print(\"\\tDel vs Mut:\\t t = \" + str(round(t_a549_del_vs_mut[0], 5)) + \"\\t p = \" + str(round(t_a549_del_vs_mut[1], 5)))\n",
    "print(\"\\tTsi vs Tvr:\\t t = \" + str(round(t_a549_tsi_vs_tvr[0], 5)) + \"\\t p = \" + str(round(t_a549_tsi_vs_tvr[1], 5)))\n",
    "print(\"\\nhek-in\")\n",
    "print(\"\\tNon vs Mut:\\t t = \" + str(round(t_hek_non_vs_mut[0], 5)) + \"\\t p = \" + str(round(t_hek_non_vs_mut[1], 5)))\n",
    "print(\"\\tNon vs Del:\\t t = \" + str(round(t_hek_non_vs_del[0], 5)) + \"\\t p = \" + str(round(t_hek_non_vs_del[1], 5)))\n",
    "print(\"\\tNon vs Tsi:\\t t = \" + str(round(t_hek_non_vs_tsi[0], 5)) + \"\\t p = \" + str(round(t_hek_non_vs_tsi[1], 5)))\n",
    "print(\"\\tNon vs Tvr:\\t t = \" + str(round(t_hek_non_vs_tvr[0], 5)) + \"\\t p = \" + str(round(t_hek_non_vs_tvr[1], 5)))\n",
    "print(\"\\tDel vs Mut:\\t t = \" + str(round(t_hek_del_vs_mut[0], 5)) + \"\\t p = \" + str(round(t_hek_del_vs_mut[1], 5)))\n",
    "print(\"\\tTsi vs Tvr:\\t t = \" + str(round(t_hek_tsi_vs_tvr[0], 5)) + \"\\t p = \" + str(round(t_hek_tsi_vs_tvr[1], 5)))\n",
    "print(\"\\nvero-in\")\n",
    "print(\"\\tNon vs Mut:\\t t = \" + str(round(t_vero_non_vs_mut[0], 5)) + \"\\t p = \" + str(round(t_vero_non_vs_mut[1], 5)))\n",
    "print(\"\\tNon vs Ins:\\t t = \" + str(round(t_vero_non_vs_ins[0], 5)) + \"\\t p = \" + str(round(t_vero_non_vs_ins[1], 5)))\n",
    "print(\"\\tNon vs Tsi:\\t t = \" + str(round(t_vero_non_vs_tsi[0], 5)) + \"\\t p = \" + str(round(t_vero_non_vs_tsi[1], 5)))\n",
    "print(\"\\tNon vs Tvr:\\t t = \" + str(round(t_vero_non_vs_tvr[0], 5)) + \"\\t p = \" + str(round(t_vero_non_vs_tvr[1], 5)))\n",
    "print(\"\\tIns vs Mut:\\t t = \" + str(round(t_vero_ins_vs_mut[0], 5)) + \"\\t p = \" + str(round(t_vero_ins_vs_mut[1], 5)))\n",
    "print(\"\\tTsi vs Tvr:\\t t = \" + str(round(t_vero_tsi_vs_tvr[0], 5)) + \"\\t p = \" + str(round(t_vero_tsi_vs_tvr[1], 5)))\n",
    "print(\"\\na549-in vs vero-in\")\n",
    "print(\"\\tNon-mutated:\\t t = \" + str(round(t_non[0], 5)) + \"\\t p = \" + str(round(t_non[1], 5)))\n",
    "print(\"\\tMutated:\\t t = \" + str(round(t_mut[0], 5)) + \"\\t p = \" + str(round(t_mut[1], 5)))\n",
    "print(\"\\tIndel:  \\t t = \" + str(round(t_indel[0], 5)) + \"\\t p = \" + str(round(t_indel[1], 5)))\n",
    "print(\"\\tTransition:  \\t t = \" + str(round(t_tvr[0], 5)) + \"\\t p = \" + str(round(t_tvr[1], 5)))\n",
    "print(\"\\tTransversion:  \\t t = \" + str(round(t_tsi[0], 5)) + \"\\t p = \" + str(round(t_tsi[1], 5)))\n",
    "\n",
    "print(\"\\n\\n----- KRUSKAL-WALLIS -----\")\n",
    "print(\"\\ta549:\\t\\t \" \"H = \" + str(round(kw_a549_non_vs_mut[0], 3)), \"\\t p = \" + str(round(kw_a549_non_vs_mut[1], 3)))\n",
    "print(\"\\thek:\\t\\t \" \"H = \" + str(round(kw_hek_non_vs_mut[0], 3)), \"\\t p = \" + str(round(kw_hek_non_vs_mut[1], 3)))\n",
    "print(\"\\tvero:\\t\\t \" \"H = \" + str(round(kw_vero_non_vs_mut[0], 3)), \"\\t p = \" + str(round(kw_vero_non_vs_mut[1], 3)))\n",
    "\n",
    "print(\"\\n\\n----- K-S -----\")\n",
    "print(\"\\ta549:\\t\\t \" \"H = \" + str(round(ks_a549_non_vs_mut[0], 3)), \"\\t p = \" + str(round(ks_a549_non_vs_mut[1], 3)))\n",
    "print(\"\\thek:\\t\\t \" \"H = \" + str(round(ks_hek_non_vs_mut[0], 3)), \"\\t p = \" + str(round(ks_hek_non_vs_mut[1], 3)))\n",
    "print(\"\\tvero:\\t\\t \" \"H = \" + str(round(ks_vero_non_vs_mut[0], 3)), \"\\t p = \" + str(round(ks_vero_non_vs_mut[1], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682e0e6-2749-4e2a-9a6a-5f27d2a3ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' make x/y scatter plots ('Belgian plots') with weeks lab coloring based on shape '''\n",
    "\n",
    "%matplotlib inline\n",
    "from RNAtools import dotPlot\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "comparisons = (\n",
    "#     (hek._in, hek._in_rep),\n",
    "#     (hek._ex, hek._ex_rep),\n",
    "#     (vero._in, vero._in_rep),\n",
    "#     (vero._ex, vero._ex_rep),\n",
    "#     (a549._in_merged, vero._in_merged),\n",
    "#     (vero._in_merged, a549._in_merged),\n",
    "#     (human._ex, vero._ex_merged),\n",
    "#     (a549._in, a549._in_rep),\n",
    "#     (vero._ex_merged, human._ex),    \n",
    "#     (a549._in, a549._in_rep),\n",
    "#     (hek._ex_rep, a549._ex),\n",
    "#     (hek._in_merged, a549._in_merged),\n",
    "#     (a549._in_merged, human._ex),\n",
    "#     (human._ex, a549._in_merged),\n",
    "#     (a549._in_rep, hek._in_rep),\n",
    "#     (a549._ex, hek._ex),\n",
    "#     (a549._ex, hek._ex_rep)\n",
    "#     (human._ex, a549._in_merged),\n",
    "#     (vero._ex_merged, vero._in_merged),\n",
    "#     (vero._in_merged, vero._ex_merged),\n",
    "#     (hek._in_merged, human._ex),\n",
    "    (human._ex, hek._in_merged),\n",
    ")\n",
    "\n",
    "def belgian_scatter(comparison, scramble=False):\n",
    "    left_inches = 0.9\n",
    "    right_inches = 0.4\n",
    "\n",
    "    fig_width = 8.5\n",
    "    fig_height = 11\n",
    "    # fig_width = 10\n",
    "    # fig_width = max(7,sp_width+left_inches+right_inches)\n",
    "    num_panels = 1\n",
    "    \n",
    "#     fig_height = 10*(math.ceil(num_panels/2))\n",
    "    # fig_height = 10*5\n",
    "    # fig_height = 1*num_panels-1  #heatbar height\n",
    "    # fig_height = 300\n",
    "\n",
    "#     left_percent = left_inches/fig_width\n",
    "#     right_percent = 1-right_inches/fig_width\n",
    "\n",
    "    fig = plt.figure(figsize=(fig_width,fig_height))\n",
    "\n",
    "    #     create axes for subplots\n",
    "#     axes = []    \n",
    "#     axes += [plt.subplot(num_panels, 1, i + 1)]\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    # for i in range(len(comparisons)):\n",
    "    #     axes += [plt.subplot(num_panels, 1, i + 1)]\n",
    "\n",
    "#     plt.subplots_adjust(hspace=0.5, left=left_percent,right=right_percent)\n",
    "    \n",
    "#     y, x = comparison\n",
    "    x, y = comparison\n",
    "\n",
    "#     fx = x.profile.copy()\n",
    "#     fy = y.profile.copy()\n",
    "\n",
    "    if (\"vero\" in x.name and \"vero\" not in y.name) or (\"vero\" not in x.name and \"vero\" in y.name):\n",
    "        print(\"aligning...\")\n",
    "        alignment = SeqIO.parse(\"clustalo-I20210622-162928-0115-39849737-p1m.clustal_num\", 'clustal')\n",
    "\n",
    "        aligned_seqs = []\n",
    "\n",
    "        for record in alignment:\n",
    "            aligned_seqs += [str(record.seq)]\n",
    "    \n",
    "#         temp = [None]*2\n",
    "#         temp[0] = aligned_seqs[1]\n",
    "#         temp[1] = aligned_seqs[0]\n",
    "#         aligned_seqs = temp\n",
    "        \n",
    "        # lets make some specific aligned variables\n",
    "        aligned_profiles = [list(x.med_normed.copy()), list(y.med_normed.copy())]\n",
    "\n",
    "        \n",
    "        print(\"constructing aligned profiles...\")\n",
    "        for si, seq in enumerate(aligned_seqs):\n",
    "            for i in range(len(seq)):\n",
    "                if seq[i] == '-':\n",
    "                    aligned_profiles[si].insert(i, np.nan)\n",
    "\n",
    "        # now our profiles should match with our sequences, num needs to be updated\n",
    "#         aligned_nums = [list(range(len(aligned_seqs[0]))), list(range(len(aligned_seqs[1])))]\n",
    "\n",
    "#         nums = aligned_nums\n",
    "#         seqs = [np.array(list(aligned_seqs[0])), np.array(list(aligned_seqs[1]))]\n",
    "#         diffs = np.array(abs(data[0].med_normed - data[1].med_normed))\n",
    "        \n",
    "        fx, fy = aligned_profiles\n",
    "        \n",
    "        fx = np.array(fx)\n",
    "        fy = np.array(fy)\n",
    "        \n",
    "    else:\n",
    "        # real\n",
    "#         diffs = np.array(data[0].med_normed - data[1].med_normed)\n",
    "        # absolute\n",
    "        fx = x.med_normed.copy()\n",
    "        fy = y.med_normed.copy()\n",
    "\n",
    "    # delete all nan values (matched, of course) from the datasets\n",
    "#     nan_mask = np.isnan(fx) | np.isnan(fy)\n",
    "#     fx = np.delete(fx, nan_mask)\n",
    "#     fy = np.delete(fy, nan_mask)\n",
    "\n",
    "#     median normalize to our reference (a549._in.profile's mean)\n",
    "#     fx = fx*(np.nanmedian(a549._in.profile)/np.nanmedian(fx))\n",
    "#     fy = fy*(np.nanmedian(a549._in.profile)/np.nanmedian(fy))\n",
    "\n",
    "    # \"Alternatively we can also color by BP probability i.e BP 0-0.1 black, BP = 0.1-0.9 gray, and BP=0.9-1 red.\"\n",
    "#     pprob = np.array(dotPlot(\"../structure_stuff/Superfold/results_a549-ex-hek-ex-rep-merged_median-normalized-to-a549-in.map_793a/merged_a549-ex-hek-ex-rep-merged_median-normalized-to-a549-in.map_793a.dp\").pairingProb())\n",
    "\n",
    "#     black_mask = 0.1 > pprob\n",
    "#     yellow_mask = (0.1 <= pprob) & (0.8 > pprob)\n",
    "#     orange_mask = (0.8 <= pprob) & (0.9 > pprob)\n",
    "#     red_mask = 0.9 <= pprob\n",
    "\n",
    "    # square-in-a-square masks\n",
    "#     grey_mask = (0 > fx) | (0 > fy)\n",
    "#     black_mask = ((0 <= fx) & (0.4 > fx)) | ((0 <= fy) & (0.4 > fy))\n",
    "#     orange_mask = ((0.4 <= fx) & (0.85 > fx)) | ((0.4 <= fy) & (0.85 > fy))\n",
    "#     red_mask = (0.85 <= fx) & (0.85 <= fy)\n",
    "#     black_mask[grey_mask] = False\n",
    "#     orange_mask[black_mask] = False\n",
    "#     orange_mask[grey_mask] = False\n",
    "    \n",
    "    if scramble:\n",
    "        np.random.shuffle(fx)\n",
    "    \n",
    "    grey_mask = (0 > fx)\n",
    "    black_mask = (0 <= fx) & (0.4 > fx)\n",
    "    orange_mask = (0.4 <= fx) & (0.85 > fx)\n",
    "    red_mask = (0.85 <= fx)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#     z_paired = np.polyfit(fx_paired, fy_paired, 1)\n",
    "#     p_paired = np.poly1d(z_paired)\n",
    "\n",
    "#     z_unpaired = np.polyfit(fx_unpaired, fy_unpaired, 1)\n",
    "#     p_unpaired = np.poly1d(z_unpaired)\n",
    "\n",
    "\n",
    "#     print(f\"name: {x.name}, {y.name};  \\t slope: {round(z[0], 3)}; medians: {round(np.nanmedian(fx), 3)}, {round(np.nanmedian(fy), 3)}\")\n",
    "\n",
    "\n",
    "#     ax.plot(range(-1,6),range(-1,6),\"r--\", lw=1)    \n",
    "\n",
    "#     scatter = ax.scatter(fx, fy, alpha=.05, color='black', rasterized=True)    \n",
    "\n",
    "#     scatter_grey = ax.scatter(fx[grey_mask], fy[grey_mask], alpha=.5, color='grey', rasterized=True)\n",
    "\n",
    "\n",
    "#     scatter_red = ax.scatter(fx[red_mask], fy[red_mask], alpha=.15, color='red', rasterized=True)\n",
    "#     scatter_orange = ax.scatter(fx[orange_mask], fy[orange_mask], alpha=.15, color='orange', rasterized=True)\n",
    "#     scatter_grey = ax.scatter(fx[grey_mask], fy[grey_mask], alpha=.15, color='grey', rasterized=True)\n",
    "#     scatter_black = ax.scatter(fx[black_mask], fy[black_mask], alpha=.15, color=\".2\", rasterized=True)                 \n",
    "\n",
    "\n",
    "\n",
    "    scatter_red = ax.scatter(fx[red_mask], fy[red_mask], alpha=.15, color='red', rasterized=True)\n",
    "    scatter_orange = ax.scatter(fx[orange_mask], fy[orange_mask], alpha=.15, color='orange', rasterized=True)\n",
    "    scatter_grey = ax.scatter(fx[grey_mask], fy[grey_mask], alpha=.15, color='grey', rasterized=True)\n",
    "    scatter_black = ax.scatter(fx[black_mask], fy[black_mask], alpha=.15, color=\".2\", rasterized=True)                 \n",
    "#     scatter_unpaired_both = ax.scatter(fx_unpaired, fy_unpaired, alpha=1, color='red', rasterized=True)\n",
    "#     scatter_paired_both = ax.scatter(fx_paired, fy_paired, alpha=1, color='black', rasterized=True)\n",
    "\n",
    "\n",
    "#     scatter_paired_one = ax.scatter(x.med_normed[paired_one], y.med_normed[paired_one], alpha=1, color='orange', rasterized=True)\n",
    "\n",
    "    # delete all nan values (matched, of course) from the datasets because polyfit can't handle nans\n",
    "    nan_mask = np.isnan(fx) | np.isnan(fy)\n",
    "    fx = np.delete(fx, nan_mask)\n",
    "    fy = np.delete(fy, nan_mask)\n",
    "\n",
    "    # re-define masks according to new (no nans) dimensions\n",
    "    \n",
    "# #     original (Belgian) masks\n",
    "    grey_mask = (0 > fx)\n",
    "    black_mask = (0 <= fx) & (0.4 > fx)\n",
    "    orange_mask = (0.4 <= fx) & (0.85 > fx)\n",
    "    red_mask = (0.85 <= fx)\n",
    "    \n",
    "    # square-in-a-square masks\n",
    "#     grey_mask = (0 > fx) | (0 > fy)\n",
    "#     black_mask = ((0 <= fx) & (0.4 > fx)) | ((0 <= fy) & (0.4 > fy))\n",
    "#     orange_mask = ((0.4 <= fx) & (0.85 > fx)) | ((0.4 <= fy) & (0.85 > fy))\n",
    "#     red_mask = (0.85 <= fx) & (0.85 <= fy)\n",
    "\n",
    "#     black_mask[grey_mask] = False\n",
    "#     orange_mask[black_mask] = False\n",
    "#     orange_mask[grey_mask] = False\n",
    "    \n",
    "    grey = [fx[grey_mask], fy[grey_mask]]\n",
    "    black = [fx[black_mask], fy[black_mask]]\n",
    "    orange = [fx[orange_mask], fy[orange_mask]]\n",
    "    red = [fx[red_mask], fy[red_mask]]\n",
    "\n",
    "#     plt.plot(x, y, 'bo')\n",
    "#     plt.plot(x, a*x, 'r-')\n",
    "#     plt.show()\n",
    "\n",
    "#     z = np.polyfit(grey[0], grey[1], 1)\n",
    "#     p = np.poly1d(z)\n",
    "#     ax.plot(grey[0], p(grey[0]),\"grey\", lw=3)\n",
    "#     text = f\"$y={z[0]:0.3f}\\;x{z[1]:+0.3f}$\\n$R^2 = {r2_score(grey[1],p(grey[0])):0.3f}$\"\n",
    "#     ax.text(0.05, 0.95, text,transform=ax.transAxes,\n",
    "#                fontsize=14, verticalalignment='top', color='grey')\n",
    "\n",
    "\n",
    "    lx = fx[:,np.newaxis]\n",
    "    a, lx_res, _, _ = np.linalg.lstsq(lx, fy)\n",
    "#     ax.plot(lx, a*lx,\"--k\", lw=3)\n",
    "    text = f\"$y={a[0]:0.3f}x$\\n$R^2 = {r2_score(fy,a*fx):0.3f}$\"\n",
    "    ax.text(0.25, 0.95, text,transform=ax.transAxes,\n",
    "               fontsize=14, verticalalignment='top', color='green')\n",
    "\n",
    "    black[0] = black[0][:,np.newaxis]\n",
    "    a, k_res, _, _ = np.linalg.lstsq(black[0], black[1])\n",
    "#     z = np.polyfit(black[0], black[1], 1)\n",
    "#     p = np.poly1d(z)\n",
    "    ax.plot(black[0], a*black[0],\"black\", lw=3)\n",
    "    text = f\"$y={a[0]:0.3f}x$\\n$R^2 = {r2_score(black[1],a*black[0]):0.3f}$\"\n",
    "    ax.text(0.05, 0.95, text,transform=ax.transAxes,\n",
    "               fontsize=14, verticalalignment='top', color='.2')\n",
    "\n",
    "    orange[0] = orange[0][:,np.newaxis]\n",
    "    a, o_res, _, _ = np.linalg.lstsq(orange[0], orange[1])\n",
    "#     z = np.polyfit(orange[0], orange[1], 1)\n",
    "#     p = np.poly1d(z)\n",
    "    ax.plot(orange[0], a*orange[0],\"yellow\", lw=3)\n",
    "    text = f\"$y={a[0]:0.3f}x$\\n$R^2 = {r2_score(orange[1],a*orange[0]):0.3f}$\"\n",
    "    ax.text(0.05, 0.85, text,transform=ax.transAxes,\n",
    "               fontsize=14, verticalalignment='top', color='orange')\n",
    "\n",
    "    red[0] = red[0][:,np.newaxis]\n",
    "    a, r_res, _, _ = np.linalg.lstsq(red[0], red[1])\n",
    "#     z = np.polyfit(red[0], red[1], 1)\n",
    "#     p = np.poly1d(z)\n",
    "    ax.plot(red[0], a*red[0],\"firebrick\", lw=3)\n",
    "    text = f\"$y={a[0]:0.3f}x$\\n$R^2 = {r2_score(red[1],a*red[0]):0.3f}$\"\n",
    "    ax.text(0.05, 0.75, text,transform=ax.transAxes,\n",
    "               fontsize=14, verticalalignment='top', color='red')\n",
    "\n",
    "#     ax.plot(fx_paired, p_paired(fx_paired),\"b--\", lw=1)\n",
    "#     ax.plot(fx_unpaired, p_unpaired(fx_unpaired),\"r--\", lw=1)\n",
    "\n",
    "    ax.set_ylim(-.5, 4)\n",
    "    ax.set_xlim(-.5, 4)\n",
    "#     ax.set_title(f\"{x.name} vs {y.name}\")\n",
    "\n",
    "    ax.set_xlabel(x.name)\n",
    "    ax.set_ylabel(y.name)\n",
    "\n",
    "    if scramble:\n",
    "        ax.set_xlabel(f\"scrambled {x.name}\")\n",
    "    \n",
    "    ax.set_aspect(1)\n",
    "    \n",
    "    print(k_res/len(black), o_res/len(orange), r_res/len(red), lx_res/len(lx))\n",
    "\n",
    "#     text = f\"$y={z_paired[0]:0.3f}\\;x{z_paired[1]:+0.3f}$\\n$R^2 = {r2_score(fy_paired,p_paired(fx_paired)):0.3f}$\"\n",
    "# #     text = f\"$y={z[0]:0.3f}\\;x{z[1]:+0.3f}$\\n$R^2 = {r2_score(fy,p(fx)):0.3f}$\"\n",
    "# #     text = f\"$R^2 = {r2_score(fy,p(fx)):0.3f}$\"\n",
    "#     ax.text(0.05, 0.95, text,transform=ax.transAxes,\n",
    "#                    fontsize=14, verticalalignment='top', color='blue')\n",
    "\n",
    "#     text = f\"$y={z_unpaired[0]:0.3f}\\;x{z_unpaired[1]:+0.3f}$\\n$R^2 = {r2_score(fy_unpaired,p_unpaired(fx_unpaired)):0.3f}$\"\n",
    "\n",
    "#     ax.text(0.05, 0.85, text,transform=ax.transAxes,\n",
    "#                    fontsize=14, verticalalignment='top', color='red')\n",
    "\n",
    "    plt.savefig(f\"plots_scatters/{x.name}_vs_{y.name}_shape_scatters_Belgium_linestozero_withtotal.pdf\")\n",
    "    \n",
    "\n",
    "for c in comparisons:\n",
    "    belgian_scatter(c, scramble=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19566128-0aae-43fc-9259-4de99cce0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' make the shape skyline plots of the two replicates '''\n",
    "%matplotlib inline\n",
    "\n",
    "comparisons = (\n",
    "#     (hek._in, hek._in_rep),\n",
    "#     (hek._ex, hek._ex_rep),\n",
    "#     (vero._in, vero._in_rep),\n",
    "#     (vero._ex, vero._ex_rep),\n",
    "#     (human._ex, a549._in_merged),\n",
    "#     (a549._ex, hek._ex),\n",
    "#     (a549._ex, hek._ex_rep),\n",
    "    (vero._in, vero._in_rep),\n",
    ")\n",
    "\n",
    "\n",
    "left_inches = 0.9\n",
    "right_inches = 0.4\n",
    "fig_width = 20\n",
    "\n",
    "num_panels = len(comparisons)\n",
    "\n",
    "fig_height = 10*num_panels\n",
    "\n",
    "left_percent = left_inches/fig_width\n",
    "right_percent = 1-right_inches/fig_width\n",
    "\n",
    "fig = plt.figure(figsize=(fig_width,fig_height), facecolor='white')\n",
    "\n",
    "#     create axes for subplots\n",
    "axes = []    \n",
    "for i in range(len(comparisons)):\n",
    "    axes += [plt.subplot(num_panels, 1, i + 1)]\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5, left=left_percent,right=right_percent, top=0.98, bottom=0.02)\n",
    "\n",
    "for ax, (rep1, rep2) in zip(axes, comparisons):\n",
    "    print(f\"generating {rep1.name}...\")\n",
    "    start = 400\n",
    "    size = 75\n",
    "\n",
    "        \n",
    "    #     print(data.name)\n",
    "    \n",
    "#     p1 = rep1.worked.copy()[start:start+size]\n",
    "#     p2 = rep2.worked.copy()[start:start+size]\n",
    "#     e1 = rep1.worked_stderr.copy()[start:start+size]\n",
    "#     e2 = rep2.worked_stderr.copy()[start:start+size]\n",
    "    \n",
    "#     p1 = rep1.profile.copy()[start:start+size]\n",
    "#     p2 = rep2.profile.copy()[start:start+size]\n",
    "#     e1 = rep1.stderr.copy()[start:start+size]\n",
    "#     e2 = rep2.stderr.copy()[start:start+size]\n",
    "    \n",
    "    p1 = rep1.med_normed.copy()[start:start+size]\n",
    "    p2 = rep2.med_normed.copy()[start:start+size]\n",
    "    e1 = rep1.med_normed_stderr.copy()[start:start+size]\n",
    "    e2 = rep2.med_normed_stderr.copy()[start:start+size]\n",
    "    \n",
    "#     e1 = [np.nan]*size\n",
    "#     e2 = [np.nan]*size\n",
    "#     nan_mask = np.isnan(rep1.worked)\n",
    "    \n",
    "#     color1 = 'royalblue'\n",
    "#     color2 = 'black'\n",
    "\n",
    "    color1 = 'green'\n",
    "    color2 = 'lightgreen'\n",
    "    \n",
    "#     ax.bar(tuple(rep1.num[start:start+size]), p1, width=1, yerr=e1, ecolor=color1, capsize=6, color=color1, alpha=0.5)\n",
    "#     ax.bar(tuple(rep2.num[start:start+size]), p2, width=1, yerr=e2, ecolor=color2, capsize=6, fc=(1, 1, 1, 0), edgecolor=color2, linewidth=2)\n",
    "#      error_kw={'elinewidth':.001, 'ls':'--'}\n",
    "    \n",
    "\n",
    "    ax.step(tuple(rep1.num[start:start+size]), p1, color=color1, where='mid')\n",
    "    ax.step(tuple(rep2.num[start:start+size]), p2, color=color2, where='mid')\n",
    "#     ax.bar(tuple(rep1.num[start:start+size]), p1, alpha=0, yerr=e1)\n",
    "\n",
    "    # for bar errorbars\n",
    "    ax.errorbar(np.array(rep1.num[start:start+size])-.1, y=p1, yerr=e1, capsize=3, color=color1, fmt='none')\n",
    "    ax.errorbar(np.array(rep2.num[start:start+size])+.1, y=p2, yerr=e2, capsize=3, color=color2, fmt='none')\n",
    "    \n",
    "    # for filled errorbars\n",
    "#     ax.fill_between(tuple(rep1.num[start:start+size]), p1+e1, p1-e1, step='pre', color=color1, alpha=0.3)\n",
    "#     ax.fill_between(tuple(rep2.num[start:start+size]), p2+e2, p2-e2, step='pre', color=color2, alpha=0.3)\n",
    "\n",
    "\n",
    "    ax.set_ylim(-.2, 4)\n",
    "    ax.set_title(f\"{rep1.name} (blue) vs {rep2.name}\")\n",
    "\n",
    "    \n",
    "    plt.savefig(f\"replicate_shape_skylines_test_rev6_{start}_{end}_a549-in-combined_vs_human-ex.pdf\")\n",
    "\n",
    "# plt.scatter(hek._in_rep.med_normed, hek._in_rep.med_normed_stderr, alpha=.1, color='red')\n",
    "# plt.scatter(hek._in.med_normed, hek._in.med_normed_stderr, alpha=.1, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393d125-384d-4fdb-bb92-3f61566b1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' secondary version of skylines code that just generates a bunch of individual skyline plot files for various regions '''\n",
    "%matplotlib inline\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "comparisons = (\n",
    "#     (hek._in, hek._in_rep),\n",
    "#     (hek._ex, hek._ex_rep),\n",
    "#     (vero._in_rep, vero._in),\n",
    "#     (vero._ex_rep, vero._ex),\n",
    "#     (hek._in_merged, a549._in_merged),\n",
    "#     (a549._in, a549._in_rep),\n",
    "#     (a549._ex, hek._ex),\n",
    "#     (hek._ex_rep, a549._ex),\n",
    "#     (a549._ex, hek._ex_rep),\n",
    "#     (a549._in_merged, hek._ex_merged), \n",
    "#     (a549._in_merged, hek._in_merged), \n",
    "#     (a549._in_merged, human._ex), \n",
    "#     (vero._in_merged, vero._ex_merged),\n",
    "    (human._ex, hek._in_merged),\n",
    ")\n",
    "\n",
    "# comparisons = ((human._ex, a549._in_merged),)\n",
    "           \n",
    "# regions = []\n",
    "\n",
    "# superfold_dir = \"/home/colindt/structure_stuff/Superfold\"\n",
    "# superfold_files = os.listdir(superfold_dir)\n",
    "\n",
    "# datasets = {\n",
    "#     'vero-in': vero._in,\n",
    "#     'vero-in-replicate': vero._in_rep,\n",
    "#     'vero-in-combined': vero._in_merged,\n",
    "#     'vero-ex': vero._ex,\n",
    "#     'vero-ex-replicate': vero._ex_rep,\n",
    "#     'vero-ex-combined': vero._ex_merged,\n",
    "#     'hek-in': hek._in,\n",
    "#     'hek-in-replicate': hek._in_rep,\n",
    "#     'hek-in-combined': hek._in_merged,\n",
    "#     'hek-ex': hek._ex,\n",
    "#     'hek-ex-replicate': hek._ex_rep,\n",
    "#     'hek-ex-combined': hek._ex_merged,\n",
    "#     'a549-in': a549._in,\n",
    "#     'a549-in-replicate': a549._in_rep,\n",
    "#     'a549-in-combined': a549._in_merged,\n",
    "#     'a549-ex': a549._ex,\n",
    "#     'a549-ex-hek-ex-rep-merged': human._ex\n",
    "# }\n",
    "\n",
    "# files = []\n",
    "\n",
    "# for f in superfold_files:\n",
    "#     if 'results' not in f:\n",
    "#         continue\n",
    "\n",
    "#     print(f\"{f}...\")\n",
    "\n",
    "#     data = datasets[f.split('_')[1]]\n",
    "\n",
    "#     # if this dataset is in our reps, we need that file\n",
    "#     if data in (rep1, rep2):\n",
    "#         region_files += os.path.join(f, \"regions\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "region_files = os.listdir(\"../structure_stuff/Superfold/results_a549-in-combined_median-normalized-to-a549-in.map_9329/regions\")\n",
    "\n",
    "# regions = []\n",
    "\n",
    "# # # parse regions from region files\n",
    "# for i, f in enumerate(region_files):\n",
    "#     if f[-2:] == 'ct':\n",
    "#         f_split = f[:-3].split('_')\n",
    "#         # adding 1 to the end of the region so that I can just use its bounds to get our real region\n",
    "#         regions  += [(int(f_split[4]), int(f_split[5]) + 1)]\n",
    "\n",
    "# figure... 1?\n",
    "# regions = ((2181, 2253),\n",
    "#            (4015, 4249),\n",
    "#            (4465, 4516),\n",
    "#            (5250, 5404),\n",
    "#            (5584, 5795),\n",
    "#            (6054, 6465),\n",
    "#            (6508, 6633))\n",
    "\n",
    "# regions = ((800, 875),\n",
    "#            (6500,6575),\n",
    "#            (2400,2475),\n",
    "#            (425, 500),\n",
    "# #            (0, 3250)\n",
    "#            (3250, 6500),\n",
    "#           )\n",
    "\n",
    "# regions = ((2181, 2253),\n",
    "#            (4465, 4516),\n",
    "#            (6508, 6633),)\n",
    "\n",
    "\n",
    "\n",
    "regions = ((6250, 6325),\n",
    "           (2000, 2150),)\n",
    "\n",
    "# regions = (\n",
    "#     (400, 475),\n",
    "#     (3800,3875),\n",
    "#     (6500,6575)\n",
    "# )\n",
    "\n",
    "# figure 2 (or is this 3?)\n",
    "# regions = ((2154, 2353),\n",
    "#            (4011, 4429),\n",
    "#            (4431, 4521),\n",
    "#            (5250, 5416),\n",
    "#            (5552, 5795),\n",
    "#            (6054, 6482),\n",
    "#            (6508, 6633))\n",
    "\n",
    "left_inches = 0.9\n",
    "right_inches = 0.4\n",
    "fig_width = 20\n",
    "\n",
    "\n",
    "\n",
    "num_panels = len(comparisons)\n",
    "\n",
    "fig_height = 10*num_panels\n",
    "\n",
    "for (start, end) in regions:\n",
    "    # the ideal width ratio for skyline plots is roughly 20width/75bases, so if I multiply 20/75 by the number of bases, I should get my new width\n",
    "    fig_width = (20/75)*(end-start)\n",
    "#     fig_width = 8.5\n",
    "#     fig_height = 11\n",
    "    \n",
    "    fig = plt.figure(figsize=(fig_width,fig_height))\n",
    "    \n",
    "#     start = r[0]\n",
    "#     end = r[1]\n",
    "    \n",
    "    #     create axes for subplots\n",
    "    axes = []    \n",
    "    for i in range(len(comparisons)):\n",
    "        axes += [plt.subplot(num_panels, 1, i + 1)]\n",
    "\n",
    "    left_percent = left_inches/fig_width\n",
    "    right_percent = 1-right_inches/fig_width\n",
    "        \n",
    "    plt.subplots_adjust(hspace=0.5, left=left_percent,right=right_percent, top=0.98, bottom=0.02)\n",
    "\n",
    "    for ax, (rep1, rep2) in zip(axes, comparisons):\n",
    "        print(f\"generating {start}-{end}...\")\n",
    "        \n",
    "    #     p1 = rep1.worked.copy()[start:start+size]\n",
    "    #     p2 = rep2.worked.copy()[start:start+size]\n",
    "    #     e1 = rep1.worked_stderr.copy()[start:start+size]\n",
    "    #     e2 = rep2.worked_stderr.copy()[start:start+size]\n",
    "\n",
    "    #     p1 = rep1.profile.copy()[start:start+size]\n",
    "    #     p2 = rep2.profile.copy()[start:start+size]\n",
    "    #     e1 = rep1.stderr.copy()[start:start+size]\n",
    "    #     e2 = rep2.stderr.copy()[start:start+size]\n",
    "\n",
    "        p1 = rep1.med_normed.copy()[start:end]\n",
    "        p2 = rep2.med_normed.copy()[start:end]\n",
    "        e1 = rep1.med_normed_stderr.copy()[start:end]\n",
    "        e2 = rep2.med_normed_stderr.copy()[start:end]\n",
    "        n1 = list(rep1.num).copy()[start:end]\n",
    "        n2 = list(rep2.num).copy()[start:end]\n",
    "        \n",
    "#         for i in range(len(p1)):\n",
    "#             if np.isnan(p1[i]) or np.isnan(p2[i]):\n",
    "#                 ax.axvspan(n1[i] - .4, n1[i] + .4, color='red', alpha=0.5)\n",
    "        \n",
    "    #     e1 = [np.nan]*size\n",
    "    #     e2 = [np.nan]*size\n",
    "    #     nan_mask = np.isnan(rep1.worked)\n",
    "\n",
    "        color1 = 'darkgoldenrod'\n",
    "    \n",
    "#         color1 = 'black'\n",
    "        color2 = 'blue'\n",
    "\n",
    "        # vero in\n",
    "#         color2 = 'darkgreen'\n",
    "#         color1 = 'yellowgreen'\n",
    "        \n",
    "        # vero ex\n",
    "#         color2 = 'red'\n",
    "#         color1 = 'hotpink'\n",
    "        \n",
    "    #     ax.bar(tuple(rep1.num[start:start+size]), p1, width=1, yerr=e1, ecolor=color1, capsize=6, color=color1, alpha=0.5)\n",
    "    #     ax.bar(tuple(rep2.num[start:start+size]), p2, width=1, yerr=e2, ecolor=color2, capsize=6, fc=(1, 1, 1, 0), edgecolor=color2, linewidth=2)\n",
    "    #      error_kw={'elinewidth':.001, 'ls':'--'}\n",
    "\n",
    "\n",
    "        ax.step(tuple(rep1.num[start:end]), p1, color=color1, where='mid', label=rep1.name)\n",
    "        ax.step(tuple(rep2.num[start:end]), p2, color=color2, where='mid', label=rep2.name)\n",
    "    #     ax.bar(tuple(rep1.num[start:start+size]), p1, alpha=0, yerr=e1)\n",
    "        ax.errorbar(np.array(rep1.num[start:end])-.1, y=p1, yerr=e1, capsize=3, color=color1, fmt='none')\n",
    "        ax.errorbar(np.array(rep2.num[start:end])+.1, y=p2, yerr=e2, capsize=3, color=color2, fmt='none')\n",
    "    #     ax.fill_between(tuple(rep1.num[start:start+size]), p1+e1, p1-e1, step='pre', color=color1, alpha=0.3)\n",
    "    #     ax.fill_between(tuple(rep2.num[start:start+size]), p2+e2, p2-e2, step='pre', color=color2, alpha=0.3)\n",
    "        ax.set_ylim(-0.5, 4)\n",
    "        ax.set_xlim(start-1, end+1)\n",
    "        ax.set_title(f\"{rep1.name} vs {rep2.name} (blue)\")\n",
    "    #     ax.set_facecolor('white')\n",
    "\n",
    "        ax.legend(loc=2)\n",
    "    \n",
    "    if not os.path.exists(\"skylines\"):\n",
    "        os.mkdir(\"skylines\")\n",
    "    \n",
    "    plt.savefig(f\"plots_skylines/replicate_shape_skylines_{rep1.name}_{rep2.name}_rev6_{start}_{end}.pdf\")\n",
    "    plt.close('all')\n",
    "\n",
    "print(\"done\")\n",
    "# plt.scatter(hek._in_rep.med_normed, hek._in_rep.med_normed_stderr, alpha=.1, color='red')\n",
    "# plt.scatter(hek._in.med_normed, hek._in.med_normed_stderr, alpha=.1, color='blue')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
